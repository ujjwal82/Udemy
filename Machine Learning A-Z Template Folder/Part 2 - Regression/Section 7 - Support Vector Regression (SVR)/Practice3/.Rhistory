dataset = dataset[2:3]
# Since we have less number of data we won't have split
# Fitting Linear Regression to the dataset
# lin_reg = lm(formula = Salary ~ Level, data = dataset)
# or
lin_reg = lm(formula = Salary ~ ., data = dataset)
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_point(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Linear Regression)') +
xlab('Levels') +
ylab('Salary')
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Linear Regression)') +
xlab('Levels') +
ylab('Salary')
data.frame(Level = 6.5)
y_pred = predict(lin_reg, newdata = data.frame(Level = 6.5)
)
y_pred
y_pred = predict(lin_reg, newdata = data.frame(Level = 6.5))
dataset$Level2 = dataset$Level ^ 2
dataset$Level3 = dataset$Level ^ 3
dataset$Level4 = dataset$Level ^ 4
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Polynomial Regression)') +
xlab('Levels') +
ylab('Salary')
poly_reg = lm(formula = Salary ~ .
, data = dataset)
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Polynomial Regression)') +
xlab('Levels') +
ylab('Salary')
y_pred = predict(poly_reg, newdata = data.frame(Level = 6.5, Level2 = 6.5^2, Level3 = 6.5^3, Level4 = 6.5^4))
y_pred
dataset$Level2 = dataset$Level ^ 2
dataset$Level3 = dataset$Level ^ 3
dataset$Level4 = dataset$Level ^ 4
dataset$Level5 = dataset$Level ^ 5
poly_reg = lm(formula = Salary ~ .
, data = dataset)
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Polynomial Regression)') +
xlab('Levels') +
ylab('Salary')
y_pred = predict(poly_reg, newdata = data.frame(Level = 6.5, Level2 = 6.5^2, Level3 = 6.5^3, Level4 = 6.5^4, Level5 = 6.5^5))
y_pred
y_pred
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Since we have less number of data we won't have split
# Fitting Linear Regression to the dataset
# lin_reg = lm(formula = Salary ~ Level, data = dataset)
# or
dataset$Level2 = dataset$Level ^ 2
dataset$Level3 = dataset$Level ^ 3
dataset$Level4 = dataset$Level ^ 4
dataset$Level5 = dataset$Level ^ 5
poly_reg = lm(formula = Salary ~ .
, data = dataset)
library(ggplot2)
ggplot()+
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)), colour = 'blue') +
ggtitle('Truth vs Bluff (Polynomial Regression)') +
xlab('Levels') +
ylab('Salary')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot()+
geom_point(aes(x = x_grid, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = x_grid, y = predict(poly_reg, newdata = data.frame(levels = x_grid))), colour = 'blue') +
ggtitle('Truth vs Bluff (Regression Model)') +
xlab('Levels') +
ylab('Salary')
setwd("D:/ujjwal/Tutorial/Udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Practice")
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Fitting the Regression Model to the dataset
install.packages('e1071')
library(e1071)
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
library(e1071)
regressor = svm(formula = 'Salary ~ .',
data = dataset)
regressor = svm(formula = 'Salary ~ .',
data = dataset,
type = 'eps-regression')
library(e1071)
regressor = svm(formula = 'Salary ~ .',
dataset)
regressor = svm(formula = 'Salary ~ .',
dataset,
type = 'eps-regression')
regressor = svm(formula = 'Salary ~ .',
dataset)
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred = predict(regressor, dataset ) #.frame(Level = 6.5))
y_pred
regressor$residuals
library(e1071)
regressor = svm(formula = 'Salary ~ .',
dataset)
regressor$residuals
regressor = svm(formula = 'Salary ~ .',
data = dataset)
library(e1071)
library(e1071)
library(e1071)
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
install.packages(c("assertthat", "BH", "bit64", "colorspace", "DBI", "devtools", "digest", "dplyr", "evaluate", "formatR", "gtable", "highr", "htmltools", "htmlwidgets", "httpuv", "httr", "jsonlite", "knitr", "lazyeval", "leaflet", "manipulate", "markdown", "memoise", "mime", "munsell", "plotly", "plyr", "proto", "R6", "Rcpp", "RCurl", "reshape2", "rJava", "roxygen2", "rstudioapi", "scales", "shiny", "sp", "stringr", "tibble", "twitteR", "xtable", "yaml"))
setwd("D:/ujjwal/Tutorial/Udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Practice2")
dataset = read.csv('regression.csv')
dataset = read.csv('regression.csv', header = TRUE)
dataset
View(dataset)
summary(dataset)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'y ~ x', data = dataset)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
abline(model)
abline(model)
# Make a prediction for each X
y_pred = predict(mode, dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
dataset = read.csv('regression.csv', header = TRUE)
summary(dataset)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
rmse = function(error){
sqrt(mean(error^2))
}
error = model$residuals # same as dataset$Y - y_pred
pred_rmse = rmse(error = error)
error
svr_model = svm(Y ~ X, dataset)
library(e1071)
svr_model = svm(Y ~ X, dataset)
y_pred = predict(svr_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
points(dataset$X, y_pred, col = 'red', pch=4)
points(dataset$X, y_pred, col = 'red', pch=4)
dataset = read.csv('regression.csv', header = TRUE)
summary(dataset)
plot(dataset, pch = 16)
library(e1071)
svr_model = svm(Y ~ X, dataset)
y_pred = predict(svr_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
error = dataset$y - y_pred
pred_rmse = rmse(error = error)
#abline(model)
error = dataset$y - y_pred
y_pred
error = dataset$Y - y_pred
pred_rmse = rmse(error = error)
tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
tuneResult = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
dataset = read.csv('regression.csv', header = TRUE)
summary(dataset)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
# let's find out how good is our prediction using linear regression
# we will use Root Mean Square Error
rmse = function(error){
sqrt(mean(error^2))
}
error = model$residuals # same as dataset$Y - y_pred
pred_rmse = rmse(error = error)
# now we know that our RMMSE of our linear regression model is 5.7.
# Let's try to improve is by SVR
library(e1071)
svr_model = svm(Y ~ X, dataset)
y_pred = predict(svr_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
error = dataset$Y - y_pred
pred_rmse = rmse(error = error) # 3.157066
tuneResult = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
tuneResult = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,0.2,0.01), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
tunedModel = tuneResult$best.model
tuned_y_Pred = predict(tunedModel, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error)
setwd("D:/ujjwal/Tutorial/Udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Practice")
# Support Vector Machine (SVM)
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
library(e1071)
regressor = svm(formula = 'Salary ~ .',
data = dataset)
library(e1071)
regressor = svm('Salary ~ .', dataset)
View(dataset)
regressor = svm(Salary ~ ., dataset)
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
library(e1071)
regressor = svm(Salary ~ ., dataset)
regressor$residuals
y_pred = predict(regressor, dataset ) #.frame(Level = 6.5))
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
library(e1071)
regressor = svm(formula = Salary ~ .,
data = dataset)
regressor = svm(formula = Salary ~ .,
data = dataset,
type = eps-regression)
regressor = svm(formula = Salary ~ .,
data = dataset,
type = 'eps-regression')
y_pred = predict(regressor, dataset ) #.frame(Level = 6.5))
y_pred = predict(regressor, data.frame(Level = 6.5))
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
library(e1071)
svr_model = svm(Y ~ X, dataset)
y_pred = predict(svr_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
setwd("D:/ujjwal/Tutorial/Udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Practice2")
dataset = read.csv('regression.csv', header = TRUE)
summary(dataset)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
# let's find out how good is our prediction using linear regression
library(e1071)
svm_model = svm(Y ~ X, dataset)
y_pred = predict(svm_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
error = dataset$Y - y_pred
pred_rmse = rmse(error = error) # 3.157066
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(svm_tune)
rmse = function(error){
sqrt(mean(error^2))
}
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(svm_tune)
library(e1071)
svm_model = svm(Y ~ X, dataset)
y_pred = predict(svm_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
error = dataset$Y - y_pred
pred_rmse = rmse(error = error) # 3.157066
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(svm_tune)
plot(svm_tune)
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,0.2,0.01), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
print(svm_tune)
# Draw the Tuning graph
plot(svm_tune)
tunedModel = tuneResult$best.model
tuned_y_Pred = predict(tunedModel, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error) # 2.1296
tuned_pred_rmse
tunedModel = tuneResult$best.model
tuned_y_Pred = predict(svm_tune, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error)
tunedModel = svm_tune$best.model
tuned_y_Pred = predict(svm_tune, data = dataset)
tunedModel = svm_tune$best.model
tuned_y_Pred = predict(tunedModel, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error) # 2.1296
tuned_pred_rmse
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(svm_tune)
tunedModel = svm_tune$best.model
tuned_y_Pred = predict(tunedModel, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error) # 2.1314
tuned_pred_rmse
svm_tune = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(svm_tune)
dataset = read.csv('regression.csv', header = TRUE)
summary(dataset)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
# let's find out how good is our prediction using linear regression
# we will use Root Mean Square Error
rmse = function(error){
sqrt(mean(error^2))
}
error = model$residuals # same as dataset$Y - y_pred
pred_rmse = rmse(error = error)
# now we know that our RMMSE of our linear regression model is 5.7.
# Let's try to improve is by SVR
library(e1071)
svr_model = svm(Y ~ X, dataset)
y_pred = predict(svr_model, dataset)
points(dataset$X, y_pred, col = 'red', pch=4)
error = dataset$Y - y_pred
pred_rmse = rmse(error = error) # 3.157066
tuneResult = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,1,0.1), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
tuneResult = tune(svm, Y ~ X, data = dataset, ranges = list(epsilon = seq(0,0.2,0.01), cost = 2^2:9))
print(tuneResult)
# Draw the Tuning graph
plot(tuneResult)
tunedModel = tuneResult$best.model
tuned_y_Pred = predict(tunedModel, data = dataset)
error = dataset$Y - tuned_y_Pred
tuned_pred_rmse =  rmse(error = error)
tuned_pred_rmse
tuned_pred_rmse
print(tuneResult)
points(dataset$X, y_pred, col = 'blue', pch=4)
plot(dataset, pch = 16)
# Create a linear regression model
model = lm(formula = 'Y ~ X', data = dataset)
# Make a prediction for each X
y_pred = predict(model, dataset)
points(dataset$X, y_pred, col = 'blue', pch=4)
line(dataset$X, y_pred, col = 'blue', pch=4)
line(x = dataset$X, y = y_pred, col = 'blue', pch=4)
line(x = dataset$X, y = y_pred)
setwd("D:/ujjwal/Tutorial/Udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Practice3")
x=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
y=c(3,4,5,4,8,10,10,11,14,20,23,24,32,34,35,37,42,48,53,60)
#Create a data frame of the data
train=data.frame(x,y)
# Let’s see how our data looks like. For this we use the plot function
#Plot the dataset
plot(train,pch=16)
model = lm(x~y, tain)
model = lm(x~y, train)
abline(model)
abline(model)
model = lm(y ~ x, train)
abline(model)
x=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
y=c(3,4,5,4,8,10,10,11,14,20,23,24,32,34,35,37,42,48,53,60)
#Create a data frame of the data
train=data.frame(x,y)
# Let’s see how our data looks like. For this we use the plot function
#Plot the dataset
plot(train,pch=16)
# Linear regression
model = lm(y ~ x, train)
abline(model)
model = svm(y ~ x, train )
library(e1071)
model_svm = svm(y ~ x, train )
# use the prediction on the data
pred = predict(model_svm, newdata = train)
# use the prediction on the data
pred <- predict(model_svm, newdata = train)
# fit a SVM model
model_svm <- svm(y ~ x, train )
model_svm <- svm(y ~ x, train )
# use the prediction on the data
pred <- predict(model_svm, newdata = train)
pred
# Plot the predictions and see our model
point(train$x, pred, col="blue", pch=4)
# Plot the predictions and see our model
points(train$x, pred, col="blue", pch=4)
error <- model$residuals
lm_errors <- sqrt(mean(error^2))
lm_errors
error_2 <- train$y - pred
svm_error <- sqrt(mean(error_2 ^ 2))
svm_error
model$residuals
model_svm$residuals
x=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
y=c(3,4,5,4,8,10,10,11,14,20,23,24,32,34,35,37,42,48,53,60)
#Create a data frame of the data
train=data.frame(x,y)
#Plot the dataset
plot(train,pch=16)
# Linear regression
model = lm(y ~ x, train)
abline(model)
model
summary(model)
error <- model$residuals
lm_errors <- sqrt(mean(error^2)) # 2.696
lm_errors
library(e1071)
# fit a SVM model
model_svm <- svm(y ~ x, train )
# use the prediction on the data
pred <- predict(model_svm, newdata = train)
# Plot the predictions and see our model
points(train$x, pred, col="blue", pch=4)
error <- model$residuals
lm_errors <- sqrt(mean(error^2)) # 3.832
lm_errors
error_2 <- train$y - pred
svm_error <- sqrt(mean(error_2 ^ 2)) # 2.696
svm_error
svm_tune <- tune(svm, y ~ x, data = train,
ranges = list(epsilon = seq(0,1,0.1), cost= 2 ^(2:9))
)
svm_tune
svm_tune$best.performance
sqrt(svm_tune$best.performance)
sqrt(svm_tune$best.performance)
best_model_svm <- svm_tune$best.model
best_model_pred <- prediction(best_model_svm, train)
best_model_pred
best_model_svm <- svm_tune$best.model
best_model_pred <- prediction(best_model_svm, train)
best_model_svm <- svm_tune$best.model
best_model_pred <- predict(best_model_svm, train)
best_model_pred
best_model_error <- train$y - best_model_error
best_model_rmse <- sqrt(mean(best_model_error ^ 2))
best_model_error <- train$y - best_model_pred
best_model_rmse <- sqrt(mean(best_model_error ^ 2))
best_model_rmse
plot(svm_tune)
plot(tain)
plot(train)
plot(train, pch=16)
points(x = train$x, y = best_model_pred, col="blue", pch=4)
